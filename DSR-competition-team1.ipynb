{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deadly-product",
   "metadata": {},
   "source": [
    "## DSR Mini-Competition \n",
    "### Rossman Sales Prediction\n",
    "#### Team 1: John Enevoldsen, Sara Ghasemi, Mena Nasr\n",
    "\n",
    "This is a notebook to reproduce the results that are submitted for the competition. Data understanding and exploration and visualisations are not included in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rental-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standard-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './modules/')\n",
    "import cleaning as cln\n",
    "import feature_eng as feng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-humanitarian",
   "metadata": {},
   "source": [
    "## 1. Initial Data Preparation\n",
    "\n",
    "Read the files and make one data frame to be used for training and cross validation, and one to be used for the final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complicated-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sara/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Read the data from 'train', 'store' and 'holdout' files:\n",
    "\n",
    "full_df_train = pd.read_csv(\"./data/train.csv\")\n",
    "full_df_store = pd.read_csv(\"./data/store.csv\")\n",
    "full_df_holdout = pd.read_csv(\"./data/holdout.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "downtown-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the 'train' and 'store' data frames, to be used for training and cross validation:\n",
    "\n",
    "full_df_train_cv = cln.merge(full_df_train, full_df_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aerial-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the 'holdout' and 'store' data frames, to be used for final test:\n",
    "\n",
    "full_df_test = cln.merge(full_df_holdout, full_df_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-charles",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greater-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Cusomers column, as was instructed to us for the competition\n",
    "\n",
    "df_train_cv = cln.drop_column(full_df_train_cv, column='Customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enormous-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target cleaning: Remove the zero and null values in targt ('Sales')\n",
    "\n",
    "df_train_cv = cln.clean_targets(df_train_cv, target='Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "correct-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows before cleaning:  531983\n",
      "Total number of rows after cleaning:  425689\n"
     ]
    }
   ],
   "source": [
    "# Feature cleaning:\n",
    "# Remove the rows with null feature values if number of null values are very small.\n",
    "# Drop the whole feature column if number of null values are not small.\n",
    "# The threshold is 10%.\n",
    "# Transform the values in 'StateHoliday' column: transform 0.0 to 0 and all column to string.\n",
    "    \n",
    "df_train_cv = cln.rough_features_cleaning(df_train_cv, threshold=0.10, drop_columns=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "declared-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Open column because at this point its values are always equal to 1\n",
    "\n",
    "df_train_cv = cln.drop_column(df_train_cv, column='Open')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-pasta",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Some feature engineering (namely mean encoding) are done after split of data into training and cross validation sets, to avoid data leakage, but others (make new date features, one_hot_encoding, etc.) can be done before.\n",
    "\n",
    "For the data split, the last 3 months (from 2014-05-01 to 2014-07-31) are kept for the cross validation set and the rest (from 2013-01-01 to 2014-04-30) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "presidential-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new date features, before split into training and cross validation sets\n",
    "\n",
    "df_train_cv = feng.dates_features(df_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complete-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one hot encoding of StateHoliday, StoreType, Assortment before split\n",
    "\n",
    "df_train_cv = feng.one_hot_encoding(df_train_cv, 'StateHoliday')\n",
    "df_train_cv = feng.one_hot_encoding(df_train_cv, 'StoreType')\n",
    "df_train_cv = feng.one_hot_encoding(df_train_cv, 'Assortment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sensitive-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and cross validation sets\n",
    "\n",
    "df_train, df_cv = feng.date_split_train_test(df_train_cv, '2014-05-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advisory-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean encoding for the Store id on Sales only for the train data\n",
    "# Then map the same values from train set to the cross validation set\n",
    "\n",
    "df_train, df_cv = feng.mean_encoding(df_train, df_cv, 'Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "western-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Store columns from data sets as it is mean-encoded now\n",
    "\n",
    "df_train = cln.drop_column(df_train, column='Store')\n",
    "df_cv = cln.drop_column(df_cv, column='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-might",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-reggae",
   "metadata": {},
   "source": [
    "Models are evaluated on the **root mean square percentage error (RMSPE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "confident-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(preds, actuals):\n",
    "    preds = preds.reshape(-1)\n",
    "    actuals = actuals.reshape(-1)\n",
    "    assert preds.shape == actuals.shape\n",
    "    return 100 * np.linalg.norm((actuals - preds) / actuals) / np.sqrt(preds.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "utility-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual or True value used in the metrin in validation process: Sales from cross validation set\n",
    "\n",
    "actuals_cv = df_cv .Sales.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "republican-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and targets used for modelling and validatoin:\n",
    "\n",
    "X_train = df_train.drop(['Date','Sales'], axis=1)\n",
    "y_train = df_train.loc[:, 'Sales']\n",
    "\n",
    "X_cv = df_cv.drop(['Date','Sales'], axis=1)\n",
    "y_cv = df_cv.loc[:, 'Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-longer",
   "metadata": {},
   "source": [
    "### 4.1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-administrator",
   "metadata": {},
   "source": [
    "#### Simple mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fossil-eating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model RMSPE = 31.68%\n"
     ]
    }
   ],
   "source": [
    "# Baseline Prediction: Mean Sales of the store, taken from train set \n",
    "# (don't get confuse by seeing df_cv name! They values are mapped from train set.)\n",
    "\n",
    "preds_baseline = df_cv.Store_mean_encoded.to_numpy()\n",
    "\n",
    "print(f'Baseline Model RMSPE = {metric(preds_baseline, actuals_cv):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-documentation",
   "metadata": {},
   "source": [
    "### 4.2. Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "western-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model initialisation\n",
    "\n",
    "model_regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "considerable-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict\n",
    "\n",
    "model_regr.fit(X_train, y_train)\n",
    "preds_regr = model_regr.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "meaningful-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model RMSPE = 23.16%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "print(f'Linear Regression Model RMSPE = {metric(preds_regr, actuals_cv):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-nicaragua",
   "metadata": {},
   "source": [
    "### 4.3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "funky-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model initialisation\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=100, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "radio-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "preds_rf = model_rf.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "horizontal-details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model RMSPE = 22.59%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "print(f'Random Forest Model RMSPE = {metric(preds_rf, actuals_cv):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-thermal",
   "metadata": {},
   "source": [
    "### 4.3. XGBoost tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "advance-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model initialisation\n",
    "\n",
    "model_xgb = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "smart-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "\n",
    "model_xgb.fit(X_train, y_train)\n",
    "preds_xgb = model_xgb.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "heavy-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model RMSPE = 20.52%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "print(f'XGBoost Model RMSPE = {metric(preds_xgb, actuals_cv):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-messenger",
   "metadata": {},
   "source": [
    "## 4. Testing on the unseen data\n",
    "\n",
    "The test set (a.k.a. holdout set) gone through the same cleaning and encoding process and our best model (XGBoost) is tested on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-theology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
